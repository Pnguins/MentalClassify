{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-15T10:15:08.676181Z",
     "iopub.status.busy": "2025-02-15T10:15:08.675866Z",
     "iopub.status.idle": "2025-02-15T10:15:54.421029Z",
     "shell.execute_reply": "2025-02-15T10:15:54.420141Z",
     "shell.execute_reply.started": "2025-02-15T10:15:08.676150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting proto-plus==1.24.0.dev1\n",
      "  Downloading proto_plus-1.24.0.dev1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from proto-plus==1.24.0.dev1) (3.20.3)\n",
      "Downloading proto_plus-1.24.0.dev1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: proto-plus\n",
      "  Attempting uninstall: proto-plus\n",
      "    Found existing installation: proto-plus 1.25.0\n",
      "    Uninstalling proto-plus-1.25.0:\n",
      "      Successfully uninstalled proto-plus-1.25.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed proto-plus-1.24.0.dev1\n",
      "Collecting qdrant-client\n",
      "  Downloading qdrant_client-1.13.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.68.1)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio_tools-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.26.4)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.11.0a1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.3.0)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.1.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->qdrant-client) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->qdrant-client) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->qdrant-client) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->qdrant-client) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->qdrant-client) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21->qdrant-client) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.28.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->qdrant-client) (4.12.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.2.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21->qdrant-client) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21->qdrant-client) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21->qdrant-client) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21->qdrant-client) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21->qdrant-client) (2024.2.0)\n",
      "Downloading qdrant_client-1.13.2-py3-none-any.whl (306 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_tools-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, portalocker, grpcio, grpcio-tools, qdrant-client\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.68.1\n",
      "    Uninstalling grpcio-1.68.1:\n",
      "      Successfully uninstalled grpcio-1.68.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "proto-plus 1.24.0.dev1 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.70.0 grpcio-tools-1.70.0 portalocker-2.10.1 protobuf-5.29.3 qdrant-client-1.13.2\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (1.34.1)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.0a1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.24.0.dev1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.28.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.48.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "grpcio-tools 1.70.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "pandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tranformers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tranformers\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.11.0a1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.28.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install torch\n",
    "!pip install proto-plus==1.24.0.dev1 \n",
    "!pip install qdrant-client\n",
    "!pip install huggingface_hub\n",
    "!pip install google-generativeai\n",
    "!pip install tranformers\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install openai pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:15:54.422358Z",
     "iopub.status.busy": "2025-02-15T10:15:54.422001Z",
     "iopub.status.idle": "2025-02-15T10:16:15.430899Z",
     "shell.execute_reply": "2025-02-15T10:16:15.430003Z",
     "shell.execute_reply.started": "2025-02-15T10:15:54.422328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Filter\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "# Model related import\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create key and secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:16:15.432560Z",
     "iopub.status.busy": "2025-02-15T10:16:15.431870Z",
     "iopub.status.idle": "2025-02-15T10:16:15.804278Z",
     "shell.execute_reply": "2025-02-15T10:16:15.803405Z",
     "shell.execute_reply.started": "2025-02-15T10:16:15.432507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "QDrant_KEY = os.getenv(\"QDrant_KEY\")\n",
    "Open_AI = os.getenv(\"OpenAI\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:16:15.806528Z",
     "iopub.status.busy": "2025-02-15T10:16:15.806309Z",
     "iopub.status.idle": "2025-02-15T10:16:19.077927Z",
     "shell.execute_reply": "2025-02-15T10:16:19.076988Z",
     "shell.execute_reply.started": "2025-02-15T10:16:15.806509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8607b195cfd64d1f92b265d1689fa33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfd2023cf2041dc9096031233a9ef26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b42804584f412aa04bb497975ebfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2629ed291a434b8db308e5956bdd12f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(HF_TOKEN)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:16:19.079347Z",
     "iopub.status.busy": "2025-02-15T10:16:19.079048Z",
     "iopub.status.idle": "2025-02-15T10:16:19.097632Z",
     "shell.execute_reply": "2025-02-15T10:16:19.097031Z",
     "shell.execute_reply.started": "2025-02-15T10:16:19.079326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Set SEED ---\\\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "# --- Load and Prepare Data ---\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads data and returns texts and labels.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    texts = data[\"query\"].tolist()\n",
    "    labels = data[\"gpt-3.5-turbo\"].tolist()\n",
    "    # Remove \" Question: The answer to the question...\" from texts\n",
    "    texts = [text.split(\"Question:\")[0] for text in texts]\n",
    "    labels = [label.split(\"Reasoning:\")[0] for label in labels]\n",
    "    return texts, labels\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_results_yesno(df):\n",
    "    \"\"\"\n",
    "    Evaluates and prints the classification report based on the DataFrame.\n",
    "    Assumes the DataFrame has 'label' and 'classification' columns,\n",
    "    and maps 'yes' to 1 and 'no' to 0 in the label.\n",
    "    \"\"\"\n",
    "    if 'label' not in df.columns or 'classification' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'label' and 'classification' columns.\")\n",
    "\n",
    "    # Map labels to numerical values\n",
    "    labels = df['label'].apply(lambda x: 1 if 'yes' in x.lower() else 0).tolist()\n",
    "    predictions = df['classification'].apply(lambda x: 1 if 'yes' in str(x).lower() else 0).tolist()\n",
    "    print(classification_report(labels, predictions))\n",
    "    return classification_report(labels, predictions, output_dict=True)\n",
    "# --- Evaluation ---\n",
    "def evaluate_results_tsid(df):\n",
    "    \"\"\"\n",
    "    Evaluates and prints the classification report based on the DataFrame.\n",
    "    Assumes the DataFrame has 'label' and 'classification' columns,\n",
    "    and maps labels to numerical values.\n",
    "    \"\"\"\n",
    "    if 'label' not in df.columns or 'classification' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'label' and 'classification' columns.\")\n",
    "\n",
    "    # Map labels to numerical values\n",
    "    label_mapping = {\n",
    "        \"no mental disorders\": 0,\n",
    "        \"suicide or self-harm tendency\": 1,\n",
    "        \"depression\": 2,\n",
    "        \"ptsd\": 3\n",
    "    }\n",
    "    labels = df['label'].apply(lambda x: next((i for i, label in enumerate(label_mapping) if label in x.lower()), 0)).tolist()\n",
    "    predictions = df['classification'].apply(lambda x: next((i for i, label in enumerate(label_mapping) if x.lower() in label), 0)).tolist()\n",
    "    # Get unique classes present in the data\n",
    "    present_labels = sorted(set(labels))  # Unique label values in the dataset\n",
    "    present_names = [name for name, idx in label_mapping.items() if idx in present_labels]\n",
    "    print(\"Evaluation for Gemini API with Few-Shot:\")\n",
    "    report = classification_report(labels, predictions, zero_division=0,labels = present_labels,target_names =present_names, output_dict=True)\n",
    "    return report\n",
    "# --- Evaluation ---\n",
    "def evaluate_results_sad(df):\n",
    "    \"\"\"\n",
    "    Evaluates and prints the classification report based on the DataFrame.\n",
    "    Assumes the DataFrame has 'label' and 'classification' columns,\n",
    "    and maps labels to numerical values.\n",
    "    \"\"\"\n",
    "    if 'label' not in df.columns or 'classification' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'label' and 'classification' columns.\")\n",
    "    # Map labels to numerical values\n",
    "    label_mapping = {\n",
    "        \"other causes\": 0,\n",
    "        \"school\": 1,\n",
    "        \"financial problem\": 2,\n",
    "        \"family issues\": 3,\n",
    "        \"social relationships\": 4,\n",
    "        \"work\": 5,\n",
    "        \"health issues\": 6,\n",
    "        \"emotional turmoil\": 7,\n",
    "        \"everyday decision making\": 8\n",
    "    }\n",
    "    labels = df['label'].apply(lambda x: next((i for i, label in enumerate(label_mapping) if label in x.lower()), 0)).tolist()\n",
    "    predictions = df['classification'].apply(lambda x: next((i for i, label in enumerate(label_mapping) if x.lower() in label), 0)).tolist()\n",
    "    # Get unique classes present in the data\n",
    "    present_labels = sorted(set(labels))  # Unique label values in the dataset\n",
    "    present_names = [name for name, idx in label_mapping.items() if idx in present_labels]\n",
    "    print(\"Evaluation for Gemini API with Few-Shot:\")\n",
    "    report = classification_report(labels, predictions, zero_division=0,labels = present_labels,target_names =present_names, output_dict=True)\n",
    "    print(report)\n",
    "    return report\n",
    "# --- Set Up Qdrant Client ---\n",
    "def setup_qdrant_client(api_key, url=\"https://35612626-619b-40d3-90db-e71a27a12e38.eu-west-1-0.aws.cloud.qdrant.io:6333\"):\n",
    "    \"\"\"Initializes Qdrant client.\"\"\"\n",
    "    return QdrantClient(url=url, api_key=api_key)\n",
    "# --- RAG Retrieval ---\n",
    "def retrieve_context(qdrant_client, collection_name, query_text, vector_size, tokenizer, top_k=10):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents from Qdrant for a given query, with padding to ensure consistent vector size.\n",
    "    \"\"\"\n",
    "    if len(query_text) > vector_size:\n",
    "        query_text = query_text[:vector_size]\n",
    "    # Generate query embedding\n",
    "    query_embedding = tokenizer(query_text, return_tensors=\"pt\", truncation=True, padding=True)[\"input_ids\"].numpy().flatten().tolist()\n",
    "\n",
    "    # Pad the embedding to match the required vector size\n",
    "    padded_embedding = pad_embedding(query_embedding, vector_size)\n",
    "\n",
    "    # Search for top-k similar documents\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=padded_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    # Extract relevant text from payloads and labels\n",
    "    if len(search_result) > 0:\n",
    "        # Group the similar posts based on cosine similarity (a simple check of the text since we do not have cosine similairity value)\n",
    "        unique_posts = []\n",
    "        unique_context = []\n",
    "        for item in search_result:\n",
    "             post = item.payload.get(\"post\", \"\")\n",
    "             label = item.payload.get(\"Label\", \"\")\n",
    "             reasoning = item.payload.get(\"Reasoning\",\"\")\n",
    "             if post not in unique_posts:\n",
    "                 unique_posts.append(post)\n",
    "                 unique_context.append((post,label,reasoning))\n",
    "        # Choose between 1-3 post from different groups.\n",
    "        selected_context = random.sample(unique_context, k=min(5, len(unique_context)))\n",
    "        \n",
    "        return selected_context\n",
    "    else:\n",
    "        return []\n",
    "# --- Pad Embedding ---\n",
    "def pad_embedding(embedding, target_size):\n",
    "    \"\"\"\n",
    "    Pads an embedding to the target size with zeros.\n",
    "\n",
    "    Args:\n",
    "        embedding (list of float): The original embedding.\n",
    "        target_size (int): The desired dimensionality.\n",
    "\n",
    "    Returns:\n",
    "        list of float: Padded embedding.\n",
    "    \"\"\"\n",
    "    return np.pad(embedding, (0, max(0, target_size - len(embedding))), mode='constant').tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:46:33.634888Z",
     "iopub.status.busy": "2025-02-15T10:46:33.634510Z",
     "iopub.status.idle": "2025-02-15T10:46:33.657913Z",
     "shell.execute_reply": "2025-02-15T10:46:33.656959Z",
     "shell.execute_reply.started": "2025-02-15T10:46:33.634861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_type):\n",
    "    model = None\n",
    "    if model_type == \"Gemma\": \n",
    "        model = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"google/gemma-2-2b-it\",\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device=\"cuda\")\n",
    "    elif model_type == \"OpenAI\":\n",
    "        model = OpenAI(api_key=Open_AI)\n",
    "    elif model_type == \"Gemini\":\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "    return model\n",
    "# --- Classification with RAG ---\n",
    "def classify_with_gemini(texts, labels, prompt_template, model, qdrant_client, collection_name, tokenizer,is_RAG, dataset):\n",
    "    \"\"\"Classifies texts using Gemini API with RAG-enhanced prompts.\"\"\"\n",
    "    results = []\n",
    "    VECTOR_SIZE = 1024\n",
    "    if dataset == \"TSID\":\n",
    "        default_res = \"no mental disorders\"\n",
    "    elif dataset == \"SAD\":\n",
    "        default_res = \"other causes\"\n",
    "    else:\n",
    "        default_res = \"no\"\n",
    "    prompt_template = prompt_template + \"\\n \"\n",
    "    for i, (text, label) in tqdm(enumerate(zip(texts, labels)), total=len(texts), desc=\"Processing Texts\"):\n",
    "        # Retrieve context using RAG\n",
    "        if is_RAG:\n",
    "            retrieved_examples =\"\"\n",
    "            context = retrieve_context(qdrant_client, collection_name, text, VECTOR_SIZE, tokenizer)\n",
    "            for index, context in enumerate(context):\n",
    "                 retrieved_examples += f\"Example {index+1}:{context[0]} \\nLabel for the context: {context[1]} \\nReasoning for the context: {context[2]}\\n\"\n",
    "                 prompt_template = prompt_template.replace(\"{retrieved_example}\",retrieved_examples)\n",
    "        \n",
    "        # Replace the placeholders with actual data\n",
    "        prompt_to_send = prompt_template.replace(\"{user_input}\", text)\n",
    "\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt_to_send,\n",
    "                safety_settings={\n",
    "                    \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_NONE\",\n",
    "                    \"HARM_CATEGORY_HATE_SPEECH\": \"BLOCK_NONE\",\n",
    "                    \"HARM_CATEGORY_SEXUALLY_EXPLICIT\": \"BLOCK_NONE\",\n",
    "                    \"HARM_CATEGORY_DANGEROUS_CONTENT\": \"BLOCK_NONE\",\n",
    "                },\n",
    "            )\n",
    "            time.sleep(5)  # To respect rate limits\n",
    "\n",
    "            # Parse the response\n",
    "            response_text = response.text.strip()\n",
    "            cleaned_response = re.sub(r\"```json|```\", \"\", response_text).strip()\n",
    "            try:\n",
    "                response_json = json.loads(cleaned_response)\n",
    "                classification = response_json.get(\"classification\", default_res)  # Default to \"no\" if missing\n",
    "                reasoning = response_json.get(\"reasoning\", \"No specific indications of stress were found in the user's text.\")\n",
    "                print(f\"Processed {i+1}/{len(texts)}: Classification: {classification}, Reasoning: {reasoning[:50]}...\")  # Print output\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                classification = default_res\n",
    "                reasoning = f\"Failed to parse JSON. Error: {e}\"\n",
    "                print(f\"Processed {i+1}/{len(texts)}: Classification: {classification}, Reasoning: {reasoning}\")  # Print error\n",
    "            except Exception as e:\n",
    "                 classification = default_res\n",
    "                 reasoning = f\"Error processing response: {e}\"\n",
    "                 print(f\"Processed {i+1}/{len(texts)}: Error: {e}\")  # Print error\n",
    "\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"classification\": classification,\n",
    "                \"reasoning\": reasoning\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"classification\": \"no\",\n",
    "                \"reasoning\": f\"Error processing response: {e}\"\n",
    "            })\n",
    "            print(f\"Processed {i+1}/{len(texts)}: Error: {e}\")  # Print error\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "def classify_with_openai(texts, labels, prompt_template, pipe, qdrant_client, collection_name, tokenizer, is_RAG, dataset,quota_limit=1000):\n",
    "    \"\"\"Classifies texts using Groq API with RAG-enhanced prompts.\"\"\"\n",
    "    count = 0\n",
    "    results = []\n",
    "    VECTOR_SIZE = 1024\n",
    "    if dataset == \"TSID\":\n",
    "        default_res = \"no mental disorders\"\n",
    "    elif dataset == \"SAD\":\n",
    "        default_res = \"other causes\"\n",
    "    else :\n",
    "        default_res = \"no\"\n",
    "    class MentalOutput(BaseModel):\n",
    "        label: str\n",
    "        reasoning: str\n",
    "        \n",
    "    for text, label in tqdm(zip(texts, labels), total=len(texts)):\n",
    "        if count >= quota_limit:\n",
    "            print(\"Quota reached. Stopping inference.\")\n",
    "            break\n",
    "        count += 1\n",
    "        # # Retrieve context using RAG\n",
    "        # print(f\"Using RAG {is_RAG}\")\n",
    "        if is_RAG:\n",
    "            USER_PROMPT_TEMPLATE = \"\"\" \n",
    "            Classify the following query based on the examples provided.\n",
    "            Examples:\n",
    "            {examples}\n",
    "            Now classify this query:\n",
    "            {query}\n",
    "            \"\"\"\n",
    "            context = retrieve_context(qdrant_client, collection_name, text, VECTOR_SIZE, tokenizer)\n",
    "            example = \"\"\n",
    "            for index, context in enumerate(context):\n",
    "                 example += f\"Example {index+1}:{context[0]} \\nLabel for the context: {context[1]} \\nReasoning for the context: {context[2]}\\n\"\n",
    "                 USER_PROMPT_TEMPLATE = USER_PROMPT_TEMPLATE.replace(\"{query}\",text).replace(\"{examples}\",example)\n",
    "        else:\n",
    "            USER_PROMPT_TEMPLATE = \"\"\" \n",
    "            Now classify this query:\n",
    "            {query}\n",
    "            \"\"\"\n",
    "            USER_PROMPT_TEMPLATE = USER_PROMPT_TEMPLATE.replace(\"{query}\",text)    \n",
    "        print(prompt_template)\n",
    "        print(USER_PROMPT_TEMPLATE)\n",
    "        try:\n",
    "            completion = pipe.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt_template},\n",
    "                    {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE},\n",
    "                ],\n",
    "                response_format=MentalOutput,\n",
    "            )\n",
    "\n",
    "            time.sleep(1)  # To respect rate limits\n",
    "            event = completion.choices[0].message.parsed\n",
    "            print(f\"Raw Response: {event}\")  # For debugging\n",
    "            classification = event.label\n",
    "            reasoning = event.reasoning\n",
    "\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"classification\": classification,\n",
    "                \"reasoning\": reasoning\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response: {e}, text: {text}\")\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"classification\": default_res,\n",
    "                \"reasoning\": f\"Error processing response: {e}\"\n",
    "            })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "def classify_with_gemma(texts, labels, prompt_template, pipe, qdrant_client, collection_name, tokenizer, is_RAG,dataset ,quota_limit=1000):\n",
    "    \"\"\"Classifies texts using Groq API with RAG-enhanced prompts.\"\"\"\n",
    "    count = 0\n",
    "    results = []\n",
    "    VECTOR_SIZE = 1024\n",
    "    if dataset == \"TSID\":\n",
    "        default_res = \"no mental disorders\"\n",
    "    elif dataset == \"SAD\":\n",
    "        default_res = \"other causes\"\n",
    "    else :\n",
    "        default_res = \"no\"\n",
    "    for text, label in zip(texts, labels):\n",
    "        if count >= quota_limit:\n",
    "            print(\"Quota reached. Stopping inference.\")\n",
    "            break\n",
    "        count += 1\n",
    "        print(f\"Processing text {count}/{len(texts)}\")\n",
    "\n",
    "        # Prepare retrieved examples for multishot prompt\n",
    "        retrieved_examples = \"\"\n",
    "        if is_RAG:\n",
    "            # Retrieve context using RAG\n",
    "            context = retrieve_context(qdrant_client, collection_name, text, VECTOR_SIZE, tokenizer)\n",
    "            for index, context in enumerate(context):\n",
    "                 retrieved_examples += f\"Example {index+1}:{context[0]} \\nLabel for the context: {context[1]} \\nReasoning for the context: {context[2]}\\n\"\n",
    "                 prompt_template = prompt_template.replace(\"{retrieved_example}\",retrieved_examples)\n",
    "\n",
    "        # Replace the placeholders with actual data\n",
    "        prompt_to_send = prompt_template.replace(\"{user_input}\", text)\n",
    "        print(prompt_to_send)\n",
    "        try:\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_to_send,\n",
    "                }\n",
    "            ]\n",
    "            outputs = pipe(messages, max_new_tokens=1024)\n",
    "            \n",
    "            # time.sleep(3)  # To respect rate limits\n",
    "\n",
    "            # Parse the response\n",
    "            response_text = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "            print(f\"Raw Response: {response_text}\")  # For debugging\n",
    "            try:\n",
    "                cleaned_response = re.sub(r\"```json|```\", \"\", response_text).strip()\n",
    "                response_json = json.loads(cleaned_response)\n",
    "                classification = response_json.get(\"classification\", default_res)\n",
    "                reasoning = response_json.get(\"reasoning\", \"No reasoning provided.\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON: {e}, Response: {response_text}\")\n",
    "                classification = default_res\n",
    "                reasoning = \"Failed to parse JSON.\"\n",
    "\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"classification\": classification,\n",
    "                \"reasoning\": reasoning\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response: {e}, text: {text}\")\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": label,\n",
    "                \"classification\": default_res,\n",
    "                \"reasoning\": f\"Error processing response: {e}\"\n",
    "            })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:16:19.123070Z",
     "iopub.status.busy": "2025-02-15T10:16:19.122795Z",
     "iopub.status.idle": "2025-02-15T10:16:19.159734Z",
     "shell.execute_reply": "2025-02-15T10:16:19.159136Z",
     "shell.execute_reply.started": "2025-02-15T10:16:19.123051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/input/ai-therapy-prompt\") \n",
    "import Prompt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:31:10.873036Z",
     "iopub.status.busy": "2025-02-15T10:31:10.872748Z",
     "iopub.status.idle": "2025-02-15T10:31:10.880605Z",
     "shell.execute_reply": "2025-02-15T10:31:10.879801Z",
     "shell.execute_reply.started": "2025-02-15T10:31:10.873015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_inference(model, dataset, method):\n",
    "    isRAG = method not in [\"ZS\", \"FS\"]\n",
    "    QDRANT_COLLECTION = \"Mental_Llama\"\n",
    "    # API and Dataset Setup\n",
    "    FILE_PATH = {\n",
    "        \"DR\": \"/kaggle/input/mentallamatestdata/test_data/test_complete/DR.csv\",\n",
    "        \"SAD\": \"/kaggle/input/mentallamatestdata/test_data/test_complete/SAD.csv\",\n",
    "        \"TSID\": \"/kaggle/input/mentallamatestdata/test_data/test_complete/t-sid.csv\",\n",
    "        \"Dreaddit\": \"/kaggle/input/mentallamatestdata/test_data/test_complete/dreaddit.csv\"\n",
    "    }\n",
    "    \n",
    "    INFERENCE_FUNC = {\n",
    "        \"Gemma\": classify_with_gemma,\n",
    "        \"OpenAI\": classify_with_openai,\n",
    "        \"Gemini\": classify_with_gemini\n",
    "    }\n",
    "    \n",
    "    PROMPT_TYPE = {\n",
    "        \"DR\": {\n",
    "            \"ZS\": Prompt_model.ZS_Prompt_DR,\n",
    "            \"FS\": Prompt_model.Fewshot_Prompt_DR,\n",
    "            \"RAG\": Prompt_model.RAG_Prompt_DR,\n",
    "            \"Our\": Prompt_model.Our_Prompt_DR\n",
    "        },\n",
    "        \"Dreaddit\": {\n",
    "            \"ZS\": Prompt_model.ZS_Prompt_Dreaddit,\n",
    "            \"FS\": Prompt_model.FS_Prompt_Dreaddit,\n",
    "            \"RAG\": Prompt_model.RAG_Prompt_Dreaddit,\n",
    "            \"Our\": Prompt_model.Our_Prompt_Dreaddit\n",
    "        },\n",
    "        \"TSID\": {\n",
    "            \"ZS\": Prompt_model.ZS_Prompt_TSID,\n",
    "            \"FS\": Prompt_model.Fewshot_Prompt_TSID,\n",
    "            \"RAG\": Prompt_model.RAG_Prompt_TSID,\n",
    "            \"Our\": Prompt_model.Our_Prompt_TSID\n",
    "        },\n",
    "        \"SAD\": {\n",
    "            \"ZS\": Prompt_model.ZS_Prompt_SAD,\n",
    "            \"FS\": Prompt_model.Fewshot_Prompt_SAD,\n",
    "            \"RAG\": Prompt_model.RAG_Prompt_SAD,\n",
    "            \"Our\": Prompt_model.Our_Prompt_SAD\n",
    "        }\n",
    "    }\n",
    "    qdrant_client = setup_qdrant_client(QDrant_KEY)\n",
    "    # Load data\n",
    "    texts, labels = load_data(FILE_PATH.get(dataset))\n",
    "\n",
    "    # Ensure the model name is correct **before** loading the model\n",
    "    classify_function = INFERENCE_FUNC.get(model)\n",
    "    if classify_function is None:\n",
    "        raise ValueError(f\"Invalid model name '{model}'. Must be one of {list(INFERENCE_FUNC.keys())}\")\n",
    "\n",
    "    # Load the model\n",
    "    model_instance = load_model(model)\n",
    "\n",
    "    # Get the correct prompt\n",
    "    prompt_template = PROMPT_TYPE.get(dataset, {}).get(method)\n",
    "    if prompt_template is None:\n",
    "        raise ValueError(f\"Invalid prompt type '{method}' for dataset '{dataset}'\")\n",
    "\n",
    "    # Classify using the correct function\n",
    "    results_df = classify_function(\n",
    "        texts,\n",
    "        labels,\n",
    "        prompt_template,\n",
    "        model_instance,\n",
    "        qdrant_client,\n",
    "        QDRANT_COLLECTION,\n",
    "        tokenizer,\n",
    "        isRAG,\n",
    "        dataset\n",
    "    )\n",
    "    # Evaluate results\n",
    "    if dataset == \"SAD\":\n",
    "        eval_report = evaluate_results_sad(results_df)\n",
    "    elif dataset == \"TSID\":\n",
    "        eval_report = evaluate_results_tsid(results_df)\n",
    "    else:\n",
    "        eval_report = evaluate_results_yesno(results_df)\n",
    "\n",
    "    # Save full output with reasoning and Evaluation\n",
    "    if eval_report:\n",
    "        eval_df = pd.DataFrame(eval_report).transpose()\n",
    "        # Fix incorrect reference to `prompt` (it should be `method`)\n",
    "        eval_df.to_csv(f\"{dataset}_{model}_{method}_Results.csv\")  \n",
    "        results_df.to_csv(f\"{dataset}_{model}_{method}_DFResults.csv\")  \n",
    "\n",
    "        print(f\"Evaluation and output saved successfully for {method} method.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T10:46:37.044064Z",
     "iopub.status.busy": "2025-02-15T10:46:37.043787Z",
     "iopub.status.idle": "2025-02-15T10:54:38.637262Z",
     "shell.execute_reply": "2025-02-15T10:54:38.636523Z",
     "shell.execute_reply.started": "2025-02-15T10:46:37.044043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for Gemini_ZS_Dreaddit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 1/414 [00:06<41:27,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/414: Classification: yes, Reasoning: The user explicitly mentions post-traumatic stress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/414 [00:11<40:31,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/414: Classification: yes, Reasoning: The post expresses a heightened sense of caution a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/414 [00:17<40:15,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/414: Classification: no, Reasoning: The post provides advice on obtaining a new phone ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/414 [00:23<40:07,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/414: Classification: yes, Reasoning: The post describes preparations for an emergency e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/414 [00:29<39:53,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/414: Classification: no, Reasoning: The post describes safety procedures in a shelter ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.50      0.20      0.29         5\n",
      "weighted avg       1.00      0.40      0.57         5\n",
      "\n",
      "Evaluation and output saved successfully for ZS method.\n",
      "Running inference for Gemini_FS_Dreaddit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 1/414 [00:05<40:31,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/414: Classification: yes, Reasoning: The user expresses a sense of loss and regret rega...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/414 [00:11<40:07,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/414: Classification: yes, Reasoning: The post expresses concern about security and priv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/414 [00:17<39:48,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/414: Classification: no, Reasoning: The post provides advice on obtaining a new phone ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/414 [00:23<39:27,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/414: Classification: yes, Reasoning: The post details preparations for an emergency esc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/414 [00:28<39:26,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/414: Classification: no, Reasoning: The post describes safety procedures within a shel...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.50      0.20      0.29         5\n",
      "weighted avg       1.00      0.40      0.57         5\n",
      "\n",
      "Evaluation and output saved successfully for FS method.\n",
      "Running inference for Gemini_RAG_Dreaddit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 0/414 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/414 [00:06<43:06,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/414: Classification: yes, Reasoning: The post expresses a sense of both accomplishment ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/414 [00:12<42:21,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/414: Classification: yes, Reasoning: The post expresses a sense of caution and paranoia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/414 [00:18<42:07,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/414: Classification: no, Reasoning: The post provides practical advice on obtaining a ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/414 [00:24<41:52,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/414: Classification: yes, Reasoning: The post describes meticulous planning for escape ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/414 [00:30<41:55,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/414: Classification: no, Reasoning: The post describes security measures in a shelter ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57         5\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.40         5\n",
      "   macro avg       0.50      0.20      0.29         5\n",
      "weighted avg       1.00      0.40      0.57         5\n",
      "\n",
      "Evaluation and output saved successfully for RAG method.\n",
      "Running inference for Gemini_Our_Dreaddit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 0/414 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/414 [00:06<44:08,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/414: Classification: yes, Reasoning: [{'condition': 'Post-Traumatic Stress Disorder (PTSD)', 'reasoning': \"The phrase 'post PTSD' directly states the presence of PTSD.\"}, {'condition': 'Stress', 'reasoning': 'The statement, \"This is good and bad\", indicates a potential stressor and internal conflict.  The phrase, \"challenging and high paid career\" suggests a previous life that is contrasted with a current situation implying potential stress from a life change.'}]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/414 [00:12<42:30,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/414: Classification: yes, Reasoning: Stress is indicated by the phrases \"Be careful of ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/414 [00:18<41:02,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/414: Classification: no, Reasoning: No indications of mental health issues were found....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/414 [00:23<40:21,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/414: Classification: no, Reasoning: No indications of mental health issues were found....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/414 [00:29<40:34,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/414: Classification: no, Reasoning: No indications of mental health issues were found....\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.30      0.37         5\n",
      "weighted avg       1.00      0.60      0.75         5\n",
      "\n",
      "Evaluation and output saved successfully for Our method.\n",
      "Running inference for Gemini_ZS_DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 1/405 [00:05<39:45,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/405: Classification: no, Reasoning: The post expresses concern about the safety of pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/405 [00:11<40:06,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/405: Classification: no, Reasoning: The provided text \"Dini ticaret haline getirenler\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/405 [00:17<40:05,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/405: Classification: no, Reasoning: While the post details a series of tragic events i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/405 [00:23<40:09,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/405: Classification: no, Reasoning: While the post expresses feelings of sadness and f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/405 [00:29<39:40,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/405: Classification: no, Reasoning: The post expresses uncertainty about future desire...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Evaluation and output saved successfully for ZS method.\n",
      "Running inference for Gemini_FS_DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 1/405 [00:05<39:09,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/405: Classification: no, Reasoning: The post expresses concern about the security of p...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/405 [00:11<39:00,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/405: Classification: no, Reasoning: The post 'Dini ticaret haline getirenler' (Those w...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/405 [00:17<39:01,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/405: Classification: no, Reasoning: The post details a family history of tragic events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/405 [00:23<39:05,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/405: Classification: no, Reasoning: While the user expresses feelings of being put on ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/405 [00:29<38:43,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/405: Classification: no, Reasoning: The user is expressing uncertainty about future de...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Evaluation and output saved successfully for FS method.\n",
      "Running inference for Gemini_RAG_DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 0/405 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/405 [00:06<42:11,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/405: Classification: no, Reasoning: The post expresses a concern about the safety of p...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/405 [00:12<41:37,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/405: Classification: no, Reasoning: The post 'Dini ticaret haline getirenler' translat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/405 [00:18<41:43,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/405: Classification: no, Reasoning: While the post describes a series of tragic events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/405 [00:24<41:23,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/405: Classification: no, Reasoning: While the poster expresses feeling hurt and put on...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/405 [00:30<41:07,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/405: Classification: no, Reasoning: The post expresses uncertainty about future desire...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.40      0.50      0.44         5\n",
      "weighted avg       0.64      0.80      0.71         5\n",
      "\n",
      "Evaluation and output saved successfully for RAG method.\n",
      "Running inference for Gemini_Our_DR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Processing Texts:   0%|          | 0/405 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/405 [00:06<41:25,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/405: Classification: no, Reasoning: The post expresses uncertainty about using a websi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/405 [00:12<40:51,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/405: Classification: no, Reasoning: The text \"Dini ticaret haline getirenler\" translat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 3/405 [00:18<40:08,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/405: Classification: yes, Reasoning: The phrase \"Depressing as fuck, isn't it?\" express...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/405 [00:24<40:44,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/405: Classification: no, Reasoning: The user expresses feelings of frustration and dis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/405 [00:30<40:54,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/405: Classification: no, Reasoning: The user expresses uncertainty about wanting child...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         4\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n",
      "Evaluation and output saved successfully for Our method.\n",
      "Running inference for Gemini_ZS_TSID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 1/959 [00:05<1:31:27,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/959: Classification: suicide or self-harm tendency, Reasoning: The post explicitly states that the user did not k...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/959 [00:11<1:31:27,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/959: Classification: no mental disorders, Reasoning: The post expresses sadness over the news of someon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/959 [00:17<1:30:41,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/959: Classification: no mental disorders, Reasoning: The post is a humorous observation about the appea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 4/959 [00:22<1:30:54,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/959: Classification: no mental disorders, Reasoning: The post expresses admiration and pride towards so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/959 [00:28<1:30:59,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/959: Classification: no mental disorders, Reasoning: The post describes an experience related to cancer...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "Evaluation and output saved successfully for ZS method.\n",
      "Running inference for Gemini_FS_TSID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 1/959 [00:05<1:32:01,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/959: Classification: suicide or self-harm tendency, Reasoning: The phrase 'I didn't kill myself' implies a recent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/959 [00:11<1:31:53,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/959: Classification: no mental disorders, Reasoning: The post expresses sadness about the death of some...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/959 [00:17<1:31:13,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/959: Classification: no mental disorders, Reasoning: The post is a quirky observation about the appeara...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 4/959 [00:22<1:30:45,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/959: Classification: no mental disorders, Reasoning: The post expresses pride and admiration for someon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/959 [00:28<1:31:17,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/959: Classification: no mental disorders, Reasoning: The post describes increased sensitivity to spirit...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "Evaluation and output saved successfully for FS method.\n",
      "Running inference for Gemini_RAG_TSID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 0/959 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/959 [00:06<1:37:07,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/959: Classification: suicide or self-harm tendency, Reasoning: The post explicitly states 'I didn't kill myself b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/959 [00:12<1:37:04,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/959: Classification: no mental disorders, Reasoning: The post expresses sadness about the news of someo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/959 [00:18<1:37:00,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/959: Classification: no mental disorders, Reasoning: The post is a humorous observation about the appea...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 4/959 [00:24<1:36:09,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/959: Classification: no mental disorders, Reasoning: The post expresses pride and admiration for someon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/959 [00:30<1:37:04,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/959: Classification: no mental disorders, Reasoning: The post describes an experience related to a canc...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "Evaluation and output saved successfully for RAG method.\n",
      "Running inference for Gemini_Our_TSID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 0/959 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/959 [00:05<1:34:36,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/959: Classification: suicide, Reasoning: The phrase \"I didn't kill myself\" directly refers ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/959 [00:11<1:35:22,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/959: Classification: no mental disorders, Reasoning: The post expresses sadness regarding the news of s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/959 [00:17<1:35:00,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/959: Classification: no mental disorders, Reasoning: The text describes a personal observation about th...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 4/959 [00:24<1:35:59,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/959: Classification: no mental disorders, Reasoning: The text expresses pride and admiration for someon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/959 [00:29<1:35:20,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/959: Classification: no mental disorders, Reasoning: The post discusses a personal experience with canc...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "Evaluation and output saved successfully for Our method.\n",
      "Running inference for Gemini_ZS_SAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 1/684 [00:05<1:04:43,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/684: Classification: Financial problem, Reasoning: The post directly discusses the potential negative...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/684 [00:11<1:05:32,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/684: Classification: Work, Reasoning: The post explicitly mentions \"work in general\" as ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/684 [00:17<1:04:51,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/684: Classification: Work, Reasoning: The user expresses a desire to find another job, i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/684 [00:22<1:04:40,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/684: Classification: Other causes, Reasoning: The post only mentions \"usual stressor\" without pr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/684 [00:28<1:04:45,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/684: Classification: Other causes, Reasoning: The post explicitly mentions stress caused by bad ...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "{'financial problem': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'work': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}, 'everyday decision making': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'micro avg': {'precision': 1.0, 'recall': 0.6, 'f1-score': 0.7499999999999999, 'support': 5}, 'macro avg': {'precision': 0.6666666666666666, 'recall': 0.5555555555555555, 'f1-score': 0.6, 'support': 5}, 'weighted avg': {'precision': 0.8, 'recall': 0.6, 'f1-score': 0.68, 'support': 5}}\n",
      "Evaluation and output saved successfully for ZS method.\n",
      "Running inference for Gemini_FS_SAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 1/684 [00:05<1:06:35,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/684: Classification: school, Reasoning: The post explicitly mentions 'upcoming exams' and ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/684 [00:11<1:07:02,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/684: Classification: Work, Reasoning: The post mentions 'work in general' and 'searching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/684 [00:17<1:05:50,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/684: Classification: work, Reasoning: The post mentions wanting to find another job, ind...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/684 [00:23<1:05:09,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/684: Classification: social relationships, Reasoning: The post explicitly expresses feelings of not fitt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/684 [00:28<1:04:25,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/684: Classification: other causes, Reasoning: Failed to parse JSON. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "{'financial problem': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'work': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}, 'everyday decision making': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'micro avg': {'precision': 1.0, 'recall': 0.4, 'f1-score': 0.5714285714285715, 'support': 5}, 'macro avg': {'precision': 0.3333333333333333, 'recall': 0.2222222222222222, 'f1-score': 0.26666666666666666, 'support': 5}, 'weighted avg': {'precision': 0.6, 'recall': 0.4, 'f1-score': 0.4800000000000001, 'support': 5}}\n",
      "Evaluation and output saved successfully for FS method.\n",
      "Running inference for Gemini_RAG_SAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 0/684 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/684 [00:05<1:08:12,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/684: Classification: Financial problem, Reasoning: The post focuses on the potential negative economi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/684 [00:12<1:09:40,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/684: Classification: work, Reasoning: The post explicitly mentions \"work in general\" as ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/684 [00:18<1:09:14,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/684: Classification: Work, Reasoning: The post directly states that the user has been wa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/684 [00:24<1:08:58,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/684: Classification: other causes, Reasoning: The post only states \"The usual stressor in my lif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/684 [00:30<1:09:02,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/684: Classification: other causes, Reasoning: The post describes stress caused by bad drivers ne...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "{'financial problem': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'work': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}, 'everyday decision making': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'micro avg': {'precision': 1.0, 'recall': 0.6, 'f1-score': 0.7499999999999999, 'support': 5}, 'macro avg': {'precision': 0.6666666666666666, 'recall': 0.5555555555555555, 'f1-score': 0.6, 'support': 5}, 'weighted avg': {'precision': 0.8, 'recall': 0.6, 'f1-score': 0.68, 'support': 5}}\n",
      "Evaluation and output saved successfully for RAG method.\n",
      "Running inference for Gemini_Our_SAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Texts:   0%|          | 0/684 [00:00<?, ?it/s]<ipython-input-5-0fb88bc642c4>:107: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n",
      "Processing Texts:   0%|          | 1/684 [00:05<1:07:37,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/684: Classification: no, Reasoning: The post focuses on the potential economic impact ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 2/684 [00:11<1:08:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/684: Classification: no, Reasoning: The post mentions \"work in general and searching t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   0%|          | 3/684 [00:18<1:08:17,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3/684: Classification: no, Reasoning: The user states \"i have been wanting to find anoth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 4/684 [00:24<1:08:34,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4/684: Classification: no, Reasoning: The post only states \"The usual stressor in my lif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts:   1%|          | 5/684 [00:30<1:08:09,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/684: Classification: no, Reasoning: The post mentions \"stressed out\" due to \"bad drive...\n",
      "Evaluation for Gemini API with Few-Shot:\n",
      "{'financial problem': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'work': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 3.0}, 'everyday decision making': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'micro avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5.0}, 'macro avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5.0}, 'weighted avg': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5.0}}\n",
      "Evaluation and output saved successfully for Our method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\"Gemma\",\"OpenAI\",\"Gemini\"]\n",
    "datasets = [\"Dreaddit\",\"DR\",\"TSID\",\"SAD\"]\n",
    "methods = [\"ZS\",\"FS\",\"RAG\",\"Our\"]\n",
    "\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        for method in methods:\n",
    "            print(f\"Running inference for {model}_{method}_{dataset}\")\n",
    "            run_inference(model, dataset,method)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6543618,
     "sourceId": 10574445,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6668025,
     "sourceId": 10755774,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
